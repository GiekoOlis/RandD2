{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Architecture for single vertebra segmentation on crops received from YOLOv5\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model input\n",
    "\n",
    "source data: dicom image contains full spine \n",
    "destination data: crop of single vertebrae (C3-C7, Th1-Th12, L1-L5, S1) ## C2 too different from others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal image size: 4736 1920\n",
      "(256, 256) (128, 128) (64, 64) (32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(\"optimal image size: 4736 1920\")\n",
    "w = 256\n",
    "h = 256\n",
    "sizes = [(h, w), (h//2, w//2), (h//4, w//4), (h//8, w//8)]\n",
    "print(sizes[0], sizes[1], sizes[2], sizes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import albumentations\n",
    "import torch\n",
    "import os\n",
    "from  functionality import *\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_resized\\data_single_vertebra\"\n",
    "\n",
    "path_to_images = os.path.join(path_to_dataset, \"images\")\n",
    "path_to_labels = os.path.join(path_to_dataset, \"labels\")\n",
    "\n",
    "path_to_train = \"\"\n",
    "path_to_val = \"\"\n",
    "path_to_test = \"\"\n",
    "\n",
    "\n",
    "all_cases = os.listdir(path_to_images)\n",
    "# all_cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal image size: 192 175\n",
      "max mask size:  285 315\n",
      "min mask size:  69 81\n",
      "min image size present [83 95]\n",
      "max image size present [299 328]\n",
      "optimal image size: 192 160\n"
     ]
    }
   ],
   "source": [
    "width_max = 0\n",
    "height_max = 0\n",
    "\n",
    "width_min = 0\n",
    "height_min = 0\n",
    "shapes = []\n",
    "for case in all_cases:\n",
    "    current_path = os.path.join(path_to_labels, case)\n",
    "\n",
    "    mask = read_mask(current_path)\n",
    "    # print(list(mask.shape)[0:2] )\n",
    "    # shapes.append(list(mask.shape[0]))\n",
    "    xx, yy, zz = mask.nonzero()\n",
    "\n",
    "\n",
    "    width = xx.max() - xx.min() + 16\n",
    "    height = yy.max() - yy.min() + 16\n",
    "\n",
    "    width_max = width if width > width_max else width_max\n",
    "    height_max = height if height > height_max else height_max\n",
    "\n",
    "    if case == '001_SD_C3.png':\n",
    "        width_min = width\n",
    "        height_min = height\n",
    "    else:\n",
    "        width_min = width if width < width_min else width_min\n",
    "        height_min = height if height < height_min else height_min\n",
    "\n",
    "\n",
    "    shapes.append(list(mask.shape)[0:2])\n",
    "\n",
    "# print(shapes)\n",
    "\n",
    "optimal_height, optimal_width = np.ceil(np.mean(np.array(shapes), axis = 0)).astype(int)\n",
    "print(\"optimal image size:\", optimal_width, optimal_height)\n",
    "# optimal_height, optimal_width = optimal_height + (64 - optimal_height % 64), optimal_width + (128 - optimal_width % 128)\n",
    "# print(\"optimal image size:\", optimal_width, optimal_height)\n",
    "\n",
    "optimal_height, optimal_width = optimal_height - optimal_height % 32, optimal_width - optimal_width % 32\n",
    "print(\"max mask size: \", width_max, height_max)\n",
    "print(\"min mask size: \", width_min, height_min)\n",
    "print(\"min image size present\", np.ceil(np.min(shapes, axis = 0)).astype(int))\n",
    "print(\"max image size present\", np.ceil(np.max(shapes, axis = 0)).astype(int))\n",
    "print(\"optimal image size:\", optimal_width, optimal_height)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class DatasetNew(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"image\": self.data[index]['image'], \"mask\": self.data[index]['mask']}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torchvision \n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations.augmentations.functional as F\n",
    "\n",
    "\n",
    "size = sizes[0]\n",
    "\n",
    "quality_augs = [\n",
    "            A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.4, alpha_coef=0.05, p=0.125), \n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, p=0.125),\n",
    "            A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, brightness_coeff=2.5, p=0.125),\n",
    "            A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.125),\n",
    "            A.CLAHE(p=0.125),\n",
    "            A.GaussNoise(var_limit=(7.0, 27.0), mean=0, per_channel=False, p=0.125),\n",
    "            A.Emboss(alpha=(0.2, 0.5), strength=(0.2, 0.7), p=0.125),\n",
    "            A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.125),\n",
    "        ]\n",
    "\n",
    "rotation_augs = [\n",
    "            A.Rotate(limit = (5,20), p=0.5),\n",
    "            A.Rotate(limit = (340,355), p=0.5)\n",
    "        ]\n",
    "\n",
    "origs = A.ReplayCompose([\n",
    "        A.OneOf([\n",
    "            A.Resize(height=size[0], width=size[1]),\n",
    "            A.Resize(height=size[0], width=size[1])], \n",
    "            p = 1),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ], p=1,\n",
    "    additional_targets={'image': 'image', 'mask': 'mask'})\n",
    "\n",
    "origs_rotared = A.ReplayCompose([\n",
    "        A.Resize(height=size[0], width=size[1]),\n",
    "        A.OneOf(rotation_augs, p=1),\n",
    "        ToTensorV2()\n",
    "    ], p=1,\n",
    "    additional_targets={'image': 'image', 'mask': 'mask'})\n",
    "\n",
    "# one_effect = A.ReplayCompose([\n",
    "#         A.Resize(height=size[0], width=size[1]),\n",
    "#         A.OneOf(quality_augs, p=1),\n",
    "#         ToTensorV2()\n",
    "#     ], p=1,\n",
    "#     additional_targets={'image': 'image', 'mask': 'mask'})\n",
    "\n",
    "one_effect_rot = A.ReplayCompose([\n",
    "        A.Resize(height=size[0], width=size[1]),\n",
    "        A.OneOf(quality_augs, p=1),\n",
    "        A.OneOf(rotation_augs, p=1),\n",
    "        ToTensorV2()\n",
    "    ], p=1,\n",
    "    additional_targets={'image': 'image', 'mask': 'mask'})    \n",
    "\n",
    "# two_effect = A.ReplayCompose([\n",
    "#         A.Resize(height=size[0], width=size[1]),\n",
    "#         A.OneOf(quality_augs, p=1),\n",
    "#         A.OneOf(quality_augs, p=1),\n",
    "#         ToTensorV2()\n",
    "#     ], p=1,\n",
    "#     additional_targets={'image': 'image', 'mask': 'mask'})\n",
    "\n",
    "# two_effect_rot = A.ReplayCompose([\n",
    "#         A.Resize(height=size[0], width=size[1]),\n",
    "#         A.OneOf(quality_augs, p=1),\n",
    "#         A.OneOf(quality_augs, p=1),\n",
    "#         A.OneOf(rotation_augs, p=1),\n",
    "#         ToTensorV2()\n",
    "#     ], p=1,\n",
    "#     additional_targets={'image': 'image', 'mask': 'mask'})\n",
    "\n",
    "dataset_augs = [origs, origs_rotared, one_effect_rot]\n",
    "# dataset_augs = [origs, origs_rotared, one_effect_rot, two_effect_rot]\n",
    "# \n",
    "\n",
    "\n",
    "val_transforms = A.ReplayCompose(\n",
    "    [   \n",
    "        A.Resize(height=size[0], width=size[1]),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={'image': 'image', 'mask': 'mask'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader:One size images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920 230\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_cases, val_cases = train_test_split(all_cases, test_size=0.2)\n",
    "print(len(train_cases), len(val_cases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "!! torch.Size([3, 256, 256]) torch.Size([256, 256])\n",
      "2760\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "from functionality import *\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# kek_mask = 0\n",
    "dataset = [[] for _ in range(len(dataset_augs))]\n",
    "valid_aug = []\n",
    "for case in train_cases:\n",
    "    # print(case)\n",
    "    path_mask = os.path.join(path_to_labels, case)\n",
    "    path_image = os.path.join(path_to_images, case)\n",
    "\n",
    "    image = cv2.imread(path_image, 1)\n",
    "    # mask = read_mask(path_mask)\n",
    "    mask = cv2.imread(path_mask, 0)   \n",
    "    mask[mask==255] = 1.0 \n",
    "    # print(np.array_equiv(mask, mask1))\n",
    "    # print((mask > 0).sum(), (mask1 > 0).sum())\n",
    "    # print(mask[mask > 0])\n",
    "    # print(mask1[mask1 > 0])\n",
    "\n",
    "    # plt.figure(figsize = (20,14))\n",
    "    # plt.subplot(1,2,1)\n",
    "    # plt.imshow(mask)\n",
    "    # plt.subplot(1,2,2)\n",
    "    # plt.imshow(mask1)\n",
    "\n",
    "    # mask[mask==255.0] = 1.0\n",
    "    # print()\n",
    "    # print(image.shape, mask.shape)\n",
    "\n",
    "    # plt.figure(figsize = (20,14))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(image)\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.imshow(mask)\n",
    "\n",
    "\n",
    "    for i, subset in enumerate(dataset_augs):\n",
    "        augmentations = subset(image=image, mask=mask)\n",
    "        # print(augmentations['image'].shape, augmentations['mask'].shape)\n",
    "        dataset[i].append({\n",
    "            \"image\": augmentations['image'],\n",
    "            \"mask\": augmentations['mask']\n",
    "        })\n",
    "        # print(image.shape, mask.shape)\n",
    "\n",
    "for case in val_cases:\n",
    "    # print(case)\n",
    "    path_mask = os.path.join(path_to_labels, case)\n",
    "    path_image = os.path.join(path_to_images, case)\n",
    "\n",
    "    image = cv2.imread(path_image, 1)\n",
    "    # mask = read_mask(path_mask)\n",
    "    mask = cv2.imread(path_mask, 0)\n",
    "    # kek_mask = mask\n",
    "    \n",
    "    # image, mask, _ = read_image(path_image, path_mask, channels3=True)\n",
    "    mask[mask==255.0] = 1.0\n",
    "\n",
    "    # print(image.shape, mask.shape)\n",
    "    # image = np.moveaxis(image, 0, 2)\n",
    "\n",
    "    \n",
    "    augmentations = val_transforms(image=image, mask=mask)\n",
    "    print(\"!!\",augmentations['image'].shape, augmentations['mask'].shape)\n",
    "\n",
    "    valid_aug.append({\n",
    "        \"image\": augmentations['image'],\n",
    "        \"mask\": augmentations['mask']\n",
    "    })\n",
    "\n",
    "\n",
    "for subset in dataset:\n",
    "    subset = DatasetNew(subset)\n",
    "\n",
    "train_dataset = torch.utils.data.ConcatDataset(dataset)\n",
    "train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size, pin_memory=True, shuffle=True)\n",
    "\n",
    "val_dataset = DatasetNew(valid_aug)\n",
    "val_loader =  torch.utils.data.DataLoader(val_dataset, batch_size, pin_memory=True, shuffle=True)\n",
    "\n",
    "\n",
    "# print(train_dataset, train_loader)\n",
    "# print(train_dataset, train_loader)\n",
    "\n",
    "# print(train_loader.__len__())\n",
    "# print(val_loader.__len__())\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(val_dataset.__len__())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    ch_sum, ch_squared_sum, count_of_batches = 0, 0, 0\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data['image'].float()\n",
    "        data /= 255        \n",
    "\n",
    "        ch_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        ch_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        count_of_batches += 1\n",
    "\n",
    "    mean = ch_sum / count_of_batches \n",
    "    std = (ch_squared_sum / count_of_batches - mean**2)**0.5\n",
    "\n",
    "    return mean, std \n",
    "\n",
    "\n",
    "def soft_dice(*, y_true, y_pred):\n",
    "    eps = 1e-15\n",
    "    y_pred = y_pred.contiguous().view(y_pred.numel())\n",
    "    y_true = y_true.contiguous().view(y_true.numel())\n",
    "    intersection = (y_pred * y_true).sum(0)\n",
    "    scores = 2. * (intersection + eps) / (y_pred.sum(0) + y_true.sum(0) + eps)\n",
    "    score = scores.sum() / scores.numel()\n",
    "    \n",
    "    return torch.clamp(score, 0., 1.)\n",
    "\n",
    "\n",
    "def hard_dice(*, y_true, y_pred, thr=0.5):\n",
    "    y_pred = (y_pred > thr).float()\n",
    "    return soft_dice(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred, thr=0.5):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    \n",
    "    y_pred = (y_pred > thr).float()\n",
    "    num_correct += (y_true == y_pred).sum()\n",
    "    num_pixels += torch.numel(y_pred)\n",
    "    \n",
    "    return num_correct/num_pixels*100\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        return 1 - soft_dice(y_true=target, y_pred=torch.sigmoid(inputs))\n",
    "\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, dice_weight=0.5):\n",
    "        super().__init__()\n",
    "        self._dice = DiceLoss()\n",
    "        self._dice_weight = dice_weight\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        return (1 - self._dice_weight) * nn.BCEWithLogitsLoss()(inputs, target) + \\\n",
    "            self._dice_weight * self._dice(inputs, target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Fyq3h3IGmN6G"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKZp8NwH9gpf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvLRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.batchNorm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DoubleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            ConvLRelu(in_channels, out_channels),\n",
    "            ConvLRelu(out_channels, out_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = DoubleConvBlock(in_channels, out_channels)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        before_pool = self.conv_block(x)\n",
    "        x = self.max_pool(before_pool)\n",
    "        return x, before_pool\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()              \n",
    "        self.conv_block = DoubleConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        return self.conv_block(torch.cat([x, y], dim=1))\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, n_filters=64):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc1 = EncoderBlock(in_channels, n_filters)\n",
    "        self.enc2 = EncoderBlock(n_filters, n_filters * 2)\n",
    "        self.enc3 = EncoderBlock(n_filters * 2, n_filters * 4)\n",
    "        self.enc4 = EncoderBlock(n_filters * 4, n_filters * 8)\n",
    "        \n",
    "        self.center = DoubleConvBlock(n_filters * 8, n_filters * 16)\n",
    "        # self.center = DoubleConvBlock(n_filters * 4, n_filters * 8)\n",
    "\n",
    "        \n",
    "        self.dec4 = DecoderBlock(n_filters * (16 + 8), n_filters * 8)\n",
    "        self.dec3 = DecoderBlock(n_filters * (8 + 4), n_filters * 4)\n",
    "        self.dec2 = DecoderBlock(n_filters * (4 + 2), n_filters * 2)\n",
    "        self.dec1 = DecoderBlock(n_filters * (2 + 1), n_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(n_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x, enc1 = self.enc1(x)\n",
    "        x, enc2 = self.enc2(x)\n",
    "        x, enc3 = self.enc3(x)\n",
    "        x, enc4 = self.enc4(x)\n",
    "\n",
    "        center = self.center(x)\n",
    "\n",
    "        dec4 = self.dec4(center, enc4)\n",
    "        dec3 = self.dec3(dec4, enc3)\n",
    "        # dec3 = self.dec3(center, enc3)\n",
    "        dec2 = self.dec2(dec3, enc2)\n",
    "        dec1 = self.dec1(dec2, enc1)\n",
    "\n",
    "        \n",
    "        final = self.final(dec1)\n",
    "\n",
    "        return final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HOFw4e8dzWba"
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    CHECKPOINTS_PATH = 'checkpoints'\n",
    "    \n",
    "    def __init__(self, model, criterion, metric, optimizer, scheduler, config, device='cuda', start_epoch=0, loss=1.0, best_metric = float('-inf')):\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        self._criterion = criterion\n",
    "        self._metrics = metric\n",
    "        self._device = device\n",
    "        \n",
    "\n",
    "        self._epochs = config['epochs'] \n",
    "        self.start_epoch = start_epoch\n",
    "        self.saved_loss = loss\n",
    "        self._early_stopping = config['early_stopping']\n",
    "\n",
    "        \n",
    "        self._best_metric = best_metric\n",
    "        if not os.path.exists(Trainer.CHECKPOINTS_PATH):\n",
    "            os.makedirs(Trainer.CHECKPOINTS_PATH)\n",
    "\n",
    "          \n",
    "        \n",
    "    \n",
    "    def fit(self, train_loader, val_loader, save_to_drive=None):\n",
    "        passed_epochs_without_upgrades = 0\n",
    "        \n",
    "        wandb.watch(self._model, self._criterion, log='all', log_freq=3)\n",
    "        for epoch in range( self.start_epoch,  self.start_epoch + self._epochs):\n",
    "            if passed_epochs_without_upgrades > self._early_stopping:\n",
    "                return \n",
    "            \n",
    "            self._model.train()\n",
    "            train_metrics = self._run_epoch(epoch, train_loader, is_training=True)\n",
    "\n",
    "            metrics_str = []\n",
    "            for name, value in train_metrics.items():\n",
    "                metrics_str.append(f'{name}: {float(value):.5f}')\n",
    "            metrics_str = ' '.join(metrics_str)\n",
    "            print('train metrics: ' + metrics_str)\n",
    "\n",
    "\n",
    "            self._model.eval()\n",
    "            val_metrics = self._run_epoch(epoch, val_loader, is_training=False)\n",
    "\n",
    "            self._scheduler.step(val_metrics['dice'])\n",
    "            print(self._optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            metrics_str = []\n",
    "            for name, value in val_metrics.items():\n",
    "                metrics_str.append(f'{name}: {float(value):.5f}')\n",
    "            metrics_str = ' '.join(metrics_str)\n",
    "            print('val metrics: ' + metrics_str)\n",
    "            \n",
    "\n",
    "            if self._best_metric < val_metrics['dice']:\n",
    "                passed_epochs_without_upgrades = 0\n",
    "                self._best_metric = val_metrics['dice']\n",
    "                wandb.log({'saved at epoch': 1})\n",
    "                torch.save(self._model.state_dict(), os.path.join(Trainer.CHECKPOINTS_PATH, 'weights.pth'))\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': self._model.state_dict(),\n",
    "                    'optimizer': self._optimizer.state_dict(),\n",
    "                    'scheduler': self._scheduler.state_dict(),\n",
    "                    'metrics': val_metrics\n",
    "                }\n",
    "                torch.save(state, os.path.join(Trainer.CHECKPOINTS_PATH, 'state.pth'))\n",
    "\n",
    "                if save_to_drive is not None:\n",
    "                  if wandb.run.name is not None:\n",
    "                    if not os.path.exists(os.path.join(save_to_drive, wandb.run.name)):\n",
    "                        path_to_folder_in_drive = os.path.join(save_to_drive, wandb.run.name)\n",
    "                        os.makedirs(path_to_folder_in_drive)\n",
    "                  else:\n",
    "                    from datetime import datetime\n",
    "                    name = datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "                    path_to_folder_in_drive = os.path.join(save_to_drive, name)\n",
    "                    os.makedirs(path_to_folder_in_drive)\n",
    "                  \n",
    "                  torch.save(self._model.state_dict(), os.path.join(path_to_folder_in_drive, 'weights.pth'))\n",
    "                  state = {\n",
    "                      'epoch': epoch,\n",
    "                      'state_dict': self._model.state_dict(),\n",
    "                      'optimizer': self._optimizer.state_dict(),\n",
    "                      'scheduler': self._scheduler.state_dict(),\n",
    "                      'metrics': val_metrics\n",
    "                  }\n",
    "                  torch.save(state, os.path.join(path_to_folder_in_drive, 'state.pth'))\n",
    "            else:\n",
    "              wandb.log({'saved at epoch': 0})\n",
    "           \n",
    "            \n",
    "            passed_epochs_without_upgrades += 1\n",
    "    \n",
    "    def _run_epoch(self, epoch, loader, is_training):\n",
    "        if is_training:\n",
    "            pbar = tqdm.tqdm(enumerate(loader), total=len(loader), desc=f'Epoch {epoch}')\n",
    "        else:\n",
    "            pbar = enumerate(loader)\n",
    "        \n",
    "        avg_metrics = defaultdict(float)\n",
    "        index = np.random.randint(0, len(loader))\n",
    "        for i, data in pbar:\n",
    "            if i == index:\n",
    "                images = data['image'].float().to(self._device)\n",
    "                y_true = data['mask'].float().unsqueeze(1).to(self._device)\n",
    "                y_pred = self._model(images)\n",
    "                \n",
    "            batch_metrics = self._step(data, is_training)\n",
    "            for name, val in batch_metrics.items():\n",
    "                avg_metrics[name] += val\n",
    "        \n",
    "        if not is_training:\n",
    "            for name, val in avg_metrics.items():\n",
    "                wandb.log({name: val / len(loader)})\n",
    "            wandb.log({'learning_rate': self._optimizer.param_groups[0]['lr']})\n",
    "            wandb.log({\"sample\": [wandb.Image(y_pred, caption=epoch)]})\n",
    "\n",
    "        return {name: value / len(loader) for name, value in avg_metrics.items()}\n",
    "    \n",
    "    def _step(self, data, is_training=True):\n",
    "        metrics_values = {}\n",
    "\n",
    "        images = data['image'].float().to(self._device)\n",
    "        y_true = data['mask'].float().unsqueeze(1).to(self._device)\n",
    "\n",
    "        # print(images.shape, y_true.shape)\n",
    "        \n",
    "        if is_training:\n",
    "            self._optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(is_training):\n",
    "            y_pred = self._model(images)\n",
    "            loss = self._criterion(y_pred, y_true)\n",
    "            \n",
    "            for name, func in self._metrics:\n",
    "                value = func(y_true=y_true, y_pred=torch.sigmoid(y_pred))\n",
    "                metrics_values[name] = value.item()\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                self._optimizer.step()\n",
    "        \n",
    "        metrics_values['loss'] = loss.item()\n",
    "        return metrics_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgiekoolis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\gieko/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='f540abfade248550e8bc3abee9a5b1914d2ad798')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gieko\\Dropbox\\NIITO_Vertebrae\\Scripts\\weight\\UNet_single-vertebrae\n"
     ]
    }
   ],
   "source": [
    "dir_path = os.getcwd()\n",
    "save_path = os.path.join(dir_path, *[\"weight\", \"UNet_single-vertebrae\"])\n",
    "print(save_path)\n",
    "\n",
    "# save_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"]=\"offline\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 87/87 [34:25<00:00, 23.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 95.38416 dice: 0.94802 loss: 0.07463\n",
      "0.000625\n",
      "val metrics: accuracy: 96.79739 dice: 0.96419 loss: 0.05161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 87/87 [34:08<00:00, 23.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 95.69028 dice: 0.95151 loss: 0.06922\n",
      "0.000625\n",
      "val metrics: accuracy: 96.44662 dice: 0.96084 loss: 0.05598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 87/87 [34:17<00:00, 23.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 95.91267 dice: 0.95400 loss: 0.06569\n",
      "0.000625\n",
      "val metrics: accuracy: 96.60082 dice: 0.96188 loss: 0.05409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 87/87 [34:06<00:00, 23.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 96.08880 dice: 0.95597 loss: 0.06286\n",
      "0.000625\n",
      "val metrics: accuracy: 96.44995 dice: 0.95973 loss: 0.05669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 87/87 [34:07<00:00, 23.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 96.24995 dice: 0.95782 loss: 0.06024\n",
      "Epoch 00035: reducing learning rate of group 0 to 3.1250e-04.\n",
      "0.0003125\n",
      "val metrics: accuracy: 96.29940 dice: 0.95792 loss: 0.05928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 87/87 [34:11<00:00, 23.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 96.53657 dice: 0.96100 loss: 0.05574\n",
      "0.0003125\n",
      "val metrics: accuracy: 96.65812 dice: 0.96200 loss: 0.05345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 87/87 [34:23<00:00, 23.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 96.62942 dice: 0.96207 loss: 0.05422\n",
      "0.0003125\n",
      "val metrics: accuracy: 96.61155 dice: 0.96153 loss: 0.05429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 87/87 [34:23<00:00, 23.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 96.72694 dice: 0.96317 loss: 0.05265\n",
      "0.0003125\n",
      "val metrics: accuracy: 96.60655 dice: 0.96163 loss: 0.05434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 87/87 [34:18<00:00, 23.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 96.82250 dice: 0.96426 loss: 0.05112\n",
      "Epoch 00039: reducing learning rate of group 0 to 1.5625e-04.\n",
      "0.00015625\n",
      "val metrics: accuracy: 96.50543 dice: 0.96066 loss: 0.05578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 87/87 [34:16<00:00, 23.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 96.96301 dice: 0.96583 loss: 0.04886\n",
      "0.00015625\n",
      "val metrics: accuracy: 96.53652 dice: 0.96153 loss: 0.05504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|██████████| 87/87 [34:25<00:00, 23.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.03618 dice: 0.96666 loss: 0.04768\n",
      "0.00015625\n",
      "val metrics: accuracy: 96.34126 dice: 0.95988 loss: 0.05803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 87/87 [34:20<00:00, 23.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.07733 dice: 0.96715 loss: 0.04702\n",
      "0.00015625\n",
      "val metrics: accuracy: 96.51175 dice: 0.96097 loss: 0.05577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|██████████| 87/87 [33:45<00:00, 23.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.13100 dice: 0.96772 loss: 0.04615\n",
      "Epoch 00043: reducing learning rate of group 0 to 7.8125e-05.\n",
      "7.8125e-05\n",
      "val metrics: accuracy: 96.53759 dice: 0.96133 loss: 0.05553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|██████████| 87/87 [33:45<00:00, 23.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.19623 dice: 0.96841 loss: 0.04517\n",
      "7.8125e-05\n",
      "val metrics: accuracy: 96.61918 dice: 0.96215 loss: 0.05435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 87/87 [35:31<00:00, 24.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.23317 dice: 0.96888 loss: 0.04454\n",
      "7.8125e-05\n",
      "val metrics: accuracy: 96.55685 dice: 0.96146 loss: 0.05534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|██████████| 87/87 [45:42<00:00, 31.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.25918 dice: 0.96917 loss: 0.04409\n",
      "7.8125e-05\n",
      "val metrics: accuracy: 96.57983 dice: 0.96174 loss: 0.05500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|██████████| 87/87 [39:47<00:00, 27.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.28621 dice: 0.96945 loss: 0.04371\n",
      "Epoch 00047: reducing learning rate of group 0 to 3.9063e-05.\n",
      "3.90625e-05\n",
      "val metrics: accuracy: 96.51656 dice: 0.96123 loss: 0.05575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|██████████| 87/87 [37:15<00:00, 25.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.33933 dice: 0.97006 loss: 0.04286\n",
      "3.90625e-05\n",
      "val metrics: accuracy: 96.58412 dice: 0.96201 loss: 0.05490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 87/87 [40:47<00:00, 28.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.34473 dice: 0.97015 loss: 0.04273\n",
      "3.90625e-05\n",
      "val metrics: accuracy: 96.56597 dice: 0.96158 loss: 0.05549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|██████████| 87/87 [37:36<00:00, 25.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train metrics: accuracy: 97.34664 dice: 0.97014 loss: 0.04273\n",
      "3.90625e-05\n",
      "val metrics: accuracy: 96.51352 dice: 0.96114 loss: 0.05617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47:   0%|          | 0/87 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▃▅▃▁▆▅▅▄▄▂▄▄▅▅▅▄▅▅▄</td></tr><tr><td>dice</td><td>█▄▅▃▁▆▅▅▄▅▃▄▅▆▅▅▅▆▅▅</td></tr><tr><td>learning_rate</td><td>█████▄▄▄▄▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▁▅▃▆█▃▃▃▅▄▇▅▅▃▄▄▅▄▅▅</td></tr><tr><td>saved at epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>96.51352</td></tr><tr><td>dice</td><td>0.96114</td></tr><tr><td>learning_rate</td><td>4e-05</td></tr><tr><td>loss</td><td>0.05617</td></tr><tr><td>saved at epoch</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync c:\\Users\\gieko\\Dropbox\\NIITO_Vertebrae\\Scripts\\wandb\\offline-run-20230303_012852-vyx31okk<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\offline-run-20230303_012852-vyx31okk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[18], line 59\u001b[0m\n",
      "\u001b[0;32m     57\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(model, criterion, metric, optimizer, scheduler, config, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;32m     58\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mload failed\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m---> 59\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_loader, val_loader, save_to_drive\u001b[39m=\u001b[39;49msave_path)\n",
      "\n",
      "Cell \u001b[1;32mIn[17], line 41\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, train_loader, val_loader, save_to_drive)\u001b[0m\n",
      "\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \n",
      "\u001b[0;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;32m---> 41\u001b[0m train_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_epoch(epoch, train_loader, is_training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;32m     43\u001b[0m metrics_str \u001b[39m=\u001b[39m []\n",
      "\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m name, value \u001b[39min\u001b[39;00m train_metrics\u001b[39m.\u001b[39mitems():\n",
      "\n",
      "Cell \u001b[1;32mIn[17], line 117\u001b[0m, in \u001b[0;36mTrainer._run_epoch\u001b[1;34m(self, epoch, loader, is_training)\u001b[0m\n",
      "\u001b[0;32m    114\u001b[0m     y_true \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n",
      "\u001b[0;32m    115\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model(images)\n",
      "\u001b[1;32m--> 117\u001b[0m batch_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(data, is_training)\n",
      "\u001b[0;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m batch_metrics\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;32m    119\u001b[0m     avg_metrics[name] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m val\n",
      "\n",
      "Cell \u001b[1;32mIn[17], line 141\u001b[0m, in \u001b[0;36mTrainer._step\u001b[1;34m(self, data, is_training)\u001b[0m\n",
      "\u001b[0;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(is_training):\n",
      "\u001b[1;32m--> 141\u001b[0m     y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(images)\n",
      "\u001b[0;32m    142\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_criterion(y_pred, y_true)\n",
      "\u001b[0;32m    144\u001b[0m     \u001b[39mfor\u001b[39;00m name, func \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metrics:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1212\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1209\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n",
      "\u001b[0;32m   1210\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n",
      "\u001b[1;32m-> 1212\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n",
      "\u001b[0;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "\n",
      "Cell \u001b[1;32mIn[12], line 85\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     83\u001b[0m \u001b[39m# dec3 = self.dec3(center, enc3)\u001b[39;00m\n",
      "\u001b[0;32m     84\u001b[0m dec2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec2(dec3, enc2)\n",
      "\u001b[1;32m---> 85\u001b[0m dec1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdec1(dec2, enc1)\n",
      "\u001b[0;32m     88\u001b[0m final \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal(dec1)\n",
      "\u001b[0;32m     90\u001b[0m \u001b[39mreturn\u001b[39;00m final\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[12], line 49\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x, y)\u001b[0m\n",
      "\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y):\n",
      "\u001b[0;32m     48\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39minterpolate(x, scale_factor\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_block(torch\u001b[39m.\u001b[39;49mcat([x, y], dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m, in \u001b[0;36mDoubleConvBlock.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n",
      "\u001b[1;32m---> 28\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_block(x)\n",
      "\u001b[0;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m, in \u001b[0;36mConvLRelu.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n",
      "\u001b[1;32m---> 13\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n",
      "\u001b[0;32m     14\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchNorm(x)\n",
      "\u001b[0;32m     15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n",
      "\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n",
      "\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n",
      "\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n",
      "\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "hyperparametrs = {\n",
    "    'n_filters': 32,\n",
    "    'loss_weight': 0.8,\n",
    "    'lr': 0.005,\n",
    "    # 'lr': 1e-3,\n",
    "    'epochs': 40,\n",
    "    'lr_reduce_rate': 0.5,\n",
    "    'patience': 3,\n",
    "    'early_stopping': 50,\n",
    "    'batch-size': batch_size,\n",
    "    'model': 'UNet',\n",
    "    'data': 'all cases',\n",
    "    'image_size': size[0],\n",
    "    'Description': 'Augmentations: only resize and rotate'\n",
    "}\n",
    "\n",
    "with wandb.init(project=\"NIITO-Single-Vertebrae-U-Net-Base\", entity=\"giekoolis\", config=hyperparametrs):\n",
    "    config = hyperparametrs\n",
    "    model = UNet(n_filters=config['n_filters'])\n",
    "    criterion = BCEDiceLoss(config['loss_weight'])\n",
    "    metric = [('accuracy', accuracy), ('dice', hard_dice)]\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            'max',\n",
    "            factor=config['lr_reduce_rate'],\n",
    "            patience=config['patience'],\n",
    "            verbose=True\n",
    "        )\n",
    "    try:\n",
    "        model.to(device)\n",
    "        state = torch.load('C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\Scripts\\\\weight\\\\UNet_single-vertebrae\\\\jolly-wind-17\\\\state.pth', map_location=device)\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        optimizer.load_state_dict(state['optimizer'])\n",
    "        scheduler.load_state_dict(state['scheduler'])\n",
    "        # optimizer\n",
    "        # optimizer.state_dict()[\"param_groups\"][0][\"lr\"] = config['lr']\n",
    "        \n",
    "        \n",
    "        # optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        #         optimizer,\n",
    "        #         'max',\n",
    "        #         factor=config['lr_reduce_rate'],\n",
    "        #         patience=config['patience'],\n",
    "        #         verbose=True\n",
    "        #     )\n",
    "        epoch = state['epoch']\n",
    "        loss = state['metrics']['loss']\n",
    "        dice = state['metrics']['dice']\n",
    "        # print(optimizer.state_dict[\"lr\"])\n",
    "        trainer = Trainer(model, criterion, metric, optimizer, scheduler, config, device=device, start_epoch=epoch, loss=loss, best_metric=dice)\n",
    "        print('load complete')\n",
    "    except:\n",
    "        trainer = Trainer(model, criterion, metric, optimizer, scheduler, config, device=device)\n",
    "        print('load failed')\n",
    "    trainer.fit(train_loader, val_loader, save_to_drive=save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: c:\\Users\\gieko\\Dropbox\\NIITO_Vertebrae\\Scripts\\wandb\\debug-cli.gieko.log\n",
      "Syncing: https://wandb.ai/giekoolis/NIITO-Single-Vertebrae-U-Net-Base/runs/vyx31okk ... done.\n"
     ]
    }
   ],
   "source": [
    "!wandb sync c:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/wandb/offline-run-20230303_012852-vyx31okk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.8125e-06\n"
     ]
    }
   ],
   "source": [
    "kek = optimizer.state_dict()\n",
    "print(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "# print(optimizer.state_dict()[\"lr\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "384b21feacb641c3e0314271197cb6efcbf5df7640640373c033a2a56e65c0f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
