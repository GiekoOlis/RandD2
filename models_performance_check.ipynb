{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Pipeline for trained models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal image size: 4736 1920\n",
      "(4736, 1920) (2368, 960) (1184, 480) (592, 240)\n"
     ]
    }
   ],
   "source": [
    "print(\"optimal image size: 4736 1920\")\n",
    "w = 1920\n",
    "h = 4736\n",
    "sizes = [(h, w), (h//2, w//2), (h//4, w//4), (h//8, w//8)]\n",
    "print(sizes[0], sizes[1], sizes[2], sizes[3])\n",
    "size = sizes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class DatasetNew(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\"image\": self.data[index]['image'], \"mask\": self.data[index]['mask']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "053\n",
      "057\n",
      "060\n",
      "063\n",
      "073\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from functionality import *\n",
    "\n",
    "path_to_dataset = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\Ver1_ 4736x1920\\\\NIITO_Vertebrae_Dataset_Test_resized\"\n",
    "#  \"C:\\Users\\EUgolnikova\\Dropbox\\NIITO_Vertebrae\\NIITO_Vertebrae_Dataset\\NIITO_Vertebrae_Dataset_Test\"\n",
    "\n",
    "path_to_images = os.path.join(path_to_dataset, \"images\")\n",
    "path_to_labels = os.path.join(path_to_dataset, \"labels\")\n",
    "\n",
    "test_cases = os.listdir(path_to_images)\n",
    "\n",
    "test_transforms = A.ReplayCompose(\n",
    "    [   \n",
    "        A.Resize(height=size[0], width=size[1]),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={'image': 'image', 'mask': 'mask'})\n",
    "\n",
    "\n",
    "\n",
    "test_aug = []\n",
    "for case in test_cases:\n",
    "    print(case)\n",
    "    path_mask = os.path.join(path_to_labels, *[case, \"fill_no_FH\", case + \"_SD.png\"])\n",
    "    path_image = os.path.join(path_to_images, *[case, case + \"_SD_resized.dcm\"])\n",
    "    \n",
    "    image, mask, _ = read_image(path_image, path_mask, channels3=True)\n",
    "    mask[mask==255.0] = 1.0\n",
    "    # print(image.shape, mask.shape)\n",
    "\n",
    "    # image = np.moveaxis(image, 0, 2)\n",
    "\n",
    "    \n",
    "    augmentations = test_transforms(image=image, mask=mask)\n",
    "    # print(\"!!\",augmentations['image'].shape, augmentations['mask'].shape)\n",
    "\n",
    "    test_aug.append({\n",
    "        \"image\": augmentations['image'],\n",
    "        \"mask\": augmentations['mask']\n",
    "    })\n",
    "\n",
    "\n",
    "test_dataset = DatasetNew(test_aug)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, pin_memory=True, shuffle=True)\n",
    "\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    ch_sum, ch_squared_sum, count_of_batches = 0, 0, 0\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data['image'].float()\n",
    "        data /= 255        \n",
    "\n",
    "        ch_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "        ch_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "        count_of_batches += 1\n",
    "\n",
    "    mean = ch_sum / count_of_batches \n",
    "    std = (ch_squared_sum / count_of_batches - mean**2)**0.5\n",
    "\n",
    "    return mean, std \n",
    "\n",
    "\n",
    "def soft_dice(*, y_true, y_pred):\n",
    "    eps = 1e-15\n",
    "    y_pred = y_pred.contiguous().view(y_pred.numel())\n",
    "    y_true = y_true.contiguous().view(y_true.numel())\n",
    "    intersection = (y_pred * y_true).sum(0)\n",
    "    scores = 2. * (intersection + eps) / (y_pred.sum(0) + y_true.sum(0) + eps)\n",
    "    score = scores.sum() / scores.numel()\n",
    "    \n",
    "    return torch.clamp(score, 0., 1.)\n",
    "\n",
    "\n",
    "def hard_dice(*, y_true, y_pred, thr=0.5):\n",
    "    y_pred = (y_pred > thr).float()\n",
    "    return soft_dice(y_true=y_true, y_pred=y_pred)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred, thr=0.5):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    \n",
    "    y_pred = (y_pred > thr).float()\n",
    "    num_correct += (y_true == y_pred).sum()\n",
    "    num_pixels += torch.numel(y_pred)\n",
    "    \n",
    "    return num_correct/num_pixels*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import skimage\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def read_mask(mask_name):\n",
    "    mask = (skimage.io.imread(mask_name)[:,:]==255).astype(np.uint8)*255\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "def make_blending(img_path, mask_path, alpha=0.5):\n",
    "    img, mask = read_mask(img_path), read_mask(mask_path)[:, :, 0]\n",
    "    colors = np.array([[0,0,0], [255,0,0]], np.uint8)\n",
    "    return (img*alpha + colors[mask.astype(np.int32)]*(1. - alpha)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def show_images_with_mask(image_path,  mask_path_fill, alpha=0.5):\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    orig, _m = read_mask(image_path), read_mask(mask_path_fill)\n",
    "    plt.imshow(orig)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    blend = make_blending(image_path, mask_path_fill, alpha)\n",
    "    plt.imshow(blend)\n",
    "\n",
    "\n",
    "def save_predictions_as_imgs(loader, model, thr=0.5, folder=\"/content/saved_images\", device='cpu'):\n",
    "  if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "  model.to(device=device).eval()\n",
    "#   model.eval()\n",
    "  acc = []\n",
    "  s_dice = []\n",
    "  h_dice = []\n",
    "  times = []\n",
    "  for idx, data in enumerate(loader):\n",
    "\n",
    "    x = img = data['image'].float().to(device=device)\n",
    "    img = torch.squeeze(img, 0)\n",
    "    img = img.permute(1, 2, 0)\n",
    "    y = mask = data['mask'].to(device=device)\n",
    "    mask = torch.squeeze(mask, 0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      start_time= time.time() \n",
    "      preds = torch.sigmoid(model(x))\n",
    "      preds = (preds > thr).float()\n",
    "      stop_time=time.time()\n",
    "\n",
    "    duration =stop_time - start_time\n",
    "    hours = duration // 3600\n",
    "    minutes = (duration - (hours * 3600)) // 60\n",
    "    seconds = duration - ((hours * 3600) + (minutes * 60))\n",
    "    msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "    times.append(duration)\n",
    "\n",
    "    x = x.float() / 255\n",
    "    # print(type(x), type(preds))\n",
    "    acc.append(accuracy(y, preds))\n",
    "    h_dice.append(hard_dice(y_true=y, y_pred=preds))\n",
    "    s_dice.append(soft_dice(y_true=y, y_pred=preds))\n",
    "\n",
    "    torchvision.utils.save_image(x, f\"{folder}/orig_{idx}.png\")\n",
    "    torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n",
    "    torchvision.utils.save_image(y.float(), f\"{folder}/gt_{idx}.png\")\n",
    "\n",
    "    # torchvision.utils.save_image(y, f\"{folder}/{idx}.png\")\n",
    "  \n",
    "  means = [np.mean(acc), np.mean(s_dice), np.mean(h_dice), np.mean(times)]\n",
    "  return {\"accuracy\": acc, \"soft DICE\": s_dice, \"DICE\": h_dice, \"time\": times, \"means\": means} \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_blended(path_to_folder, number_of_images, acc = None, s_dice = None, h_dice = None, alpha = 0.75, beta = 0.95):\n",
    "\n",
    "    for k in range(number_of_images):\n",
    "        path_img = os.path.join(path_to_folder, \"orig_\" + str(k) + \".png\")\n",
    "        path_mask = os.path.join(path_to_folder, \"pred_\" + str(k) + \".png\")\n",
    "        path_mask_gt = os.path.join(path_to_folder, \"gt_\" + str(k) + \".png\")\n",
    "        path_save = os.path.join(path_to_folder, \"blend_\" + str(k) + \".png\")\n",
    "\n",
    "        img = plt.imread(path_img)\n",
    "        msk = plt.imread(path_mask)\n",
    "        msk_gt = plt.imread(path_mask_gt)\n",
    "\n",
    "        red  = np.array([1,0,0],dtype=np.uint8)\n",
    "        blue = np.array([0,0,1],dtype=np.uint8)\n",
    "\n",
    "        # print(accuracy(img, msk))\n",
    "        # print(hard_dice(y_true=img, y_pred=msk))\n",
    "        # print(soft_dice(y_true=img, y_pred=msk))\n",
    "\n",
    "        # plt.figure(figsize=(20, 14))\n",
    "        # plt.subplot(1, 3, 1)\n",
    "        # plt.imshow(img)\n",
    "        # plt.subplot(1, 3, 2)\n",
    "        # plt.imshow(msk)\n",
    "\n",
    "        # print(img.shape, msk.shape)\n",
    "        for i in range(msk.shape[0]):\n",
    "            for j in range(msk.shape[1]):\n",
    "                if msk[i, j].all() == 1:\n",
    "                    msk[i, j] = red\n",
    "                if msk_gt[i, j].all() == 1:\n",
    "                    msk_gt[i, j] = blue\n",
    "\n",
    "        # for i in range(msk.shape[0]):\n",
    "        #     for j in range(msk.shape[1]):\n",
    "        #         if msk[i, j].all() == 1:\n",
    "        #             msk[i, j] = red\n",
    "                \n",
    "\n",
    "        res = (img*alpha + msk*(1 - alpha))\n",
    "        res = (res*beta + msk_gt*(1 - beta))\n",
    "\n",
    "\n",
    "        # plt.subplot(1, 3, 3)\n",
    "        # plt.imshow(res)\n",
    "\n",
    "\n",
    "        res = (res * 255).astype(np.uint8)\n",
    "\n",
    "        im = Image.fromarray(res)\n",
    "\n",
    "        str_ = \"\"\n",
    "        str_ += f\"\\n accuracy: {acc[k]}\" if acc is not None else \"\"\n",
    "        str_ += f\"\\n DICE: {h_dice[k]}\" if h_dice is not None else \"\"\n",
    "\n",
    "        # print(str_)\n",
    "        font = ImageFont.truetype('arial', size=28)\n",
    "        ImageDraw.Draw(\n",
    "            im  # Image\n",
    "        ).text(\n",
    "            (0, 0),  # Coordinates\n",
    "            str_,  # Text\n",
    "            (255, 185, 93),  # Color\n",
    "            font\n",
    "        )\n",
    "\n",
    "\n",
    "        im.save(path_save)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvLRelu(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False)\n",
    "        self.batchNorm = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DoubleConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            ConvLRelu(in_channels, out_channels),\n",
    "            ConvLRelu(out_channels, out_channels),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.conv_block = DoubleConvBlock(in_channels, out_channels)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        before_pool = self.conv_block(x)\n",
    "        x = self.max_pool(before_pool)\n",
    "        return x, before_pool\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DecoderBlock, self).__init__()              \n",
    "        self.conv_block = DoubleConvBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = nn.functional.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        return self.conv_block(torch.cat([x, y], dim=1))\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, n_filters=64):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.enc1 = EncoderBlock(in_channels, n_filters)\n",
    "        self.enc2 = EncoderBlock(n_filters, n_filters * 2)\n",
    "        self.enc3 = EncoderBlock(n_filters * 2, n_filters * 4)\n",
    "        self.enc4 = EncoderBlock(n_filters * 4, n_filters * 8)\n",
    "        \n",
    "        self.center = DoubleConvBlock(n_filters * 8, n_filters * 16)\n",
    "        \n",
    "        self.dec4 = DecoderBlock(n_filters * (16 + 8), n_filters * 8)\n",
    "        self.dec3 = DecoderBlock(n_filters * (8 + 4), n_filters * 4)\n",
    "        self.dec2 = DecoderBlock(n_filters * (4 + 2), n_filters * 2)\n",
    "        self.dec1 = DecoderBlock(n_filters * (2 + 1), n_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(n_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x, enc1 = self.enc1(x)\n",
    "        x, enc2 = self.enc2(x)\n",
    "        x, enc3 = self.enc3(x)\n",
    "        x, enc4 = self.enc4(x)\n",
    "\n",
    "        center = self.center(x)\n",
    "\n",
    "        dec4 = self.dec4(center, enc4)\n",
    "        dec3 = self.dec3(dec4, enc3)\n",
    "        dec2 = self.dec2(dec3, enc2)\n",
    "        dec1 = self.dec1(dec2, enc1)\n",
    "\n",
    "        \n",
    "        final = self.final(dec1)\n",
    "\n",
    "        return final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morning Fire 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m\n",
      "\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mC:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/weight/UNet_spinal-cord/morning-fire-16/weights.pth\u001b[39m\u001b[39m'\u001b[39m, map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "\u001b[0;32m     14\u001b[0m folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/inference/UNet_spinal-cord/morning-fire-16/\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m---> 15\u001b[0m metrics \u001b[39m=\u001b[39m save_predictions_as_imgs(test_loader, model, folder \u001b[39m=\u001b[39;49m folder)\n",
      "\n",
      "Cell \u001b[1;32mIn[8], line 51\u001b[0m, in \u001b[0;36msave_predictions_as_imgs\u001b[1;34m(loader, model, thr, folder, device)\u001b[0m\n",
      "\u001b[0;32m     49\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;32m     50\u001b[0m   start_time\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \n",
      "\u001b[1;32m---> 51\u001b[0m   preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(model(x))\n",
      "\u001b[0;32m     52\u001b[0m   preds \u001b[39m=\u001b[39m (preds \u001b[39m>\u001b[39m thr)\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[0;32m     53\u001b[0m   stop_time\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[10], line 72\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n",
      "\u001b[0;32m     71\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[1;32m---> 72\u001b[0m     x, enc1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc1(x)\n",
      "\u001b[0;32m     73\u001b[0m     x, enc2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc2(x)\n",
      "\u001b[0;32m     74\u001b[0m     x, enc3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc3(x)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[10], line 38\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n",
      "\u001b[1;32m---> 38\u001b[0m     before_pool \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_block(x)\n",
      "\u001b[0;32m     39\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(before_pool)\n",
      "\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m x, before_pool\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m, in \u001b[0;36mDoubleConvBlock.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n",
      "\u001b[1;32m---> 28\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_block(x)\n",
      "\u001b[0;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n",
      "\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n",
      "\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m, in \u001b[0;36mConvLRelu.forward\u001b[1;34m(self, x)\u001b[0m\n",
      "\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n",
      "\u001b[1;32m---> 13\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n",
      "\u001b[0;32m     14\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchNorm(x)\n",
      "\u001b[0;32m     15\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n",
      "\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\gieko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n",
      "\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n",
      "\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n",
      "\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n",
      "\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparametrs = {\n",
    "    'n_filters': 32,\n",
    "    'loss_weight': 0.8,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 50,\n",
    "    'lr_reduce_rate': 0.5,\n",
    "    'patience': 4,\n",
    "    'early_stopping': 50, # пока уберем раннюю остановку\n",
    "    'model': 'test'\n",
    "}\n",
    "model = UNet(n_filters=hyperparametrs['n_filters'])\n",
    "model.load_state_dict(torch.load('C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/weight/UNet_spinal-cord/morning-fire-16/weights.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "folder = 'C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/inference/UNet_spinal-cord/morning-fire-16/'\n",
    "metrics = save_predictions_as_imgs(test_loader, model, folder = folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Pine 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swift Sunset 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametrs = {\n",
    "    'n_filters': 32,\n",
    "    'loss_weight': 0.8,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 50,\n",
    "    'lr_reduce_rate': 0.5,\n",
    "    'patience': 4,\n",
    "    'early_stopping': 50, # пока уберем раннюю остановку\n",
    "    'model': 'test'\n",
    "}\n",
    "model = UNet(n_filters=hyperparametrs['n_filters'])\n",
    "model.load_state_dict(torch.load('C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/weight/UNet_spinal-cord/swift-sunset-18/weights.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "folder = 'C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/inference/UNet_spinal-cord/swift-sunset-18/'\n",
    "metrics = save_predictions_as_imgs(test_loader, model, folder = folder)\n",
    "\n",
    "save_blended(path_to_folder=folder, number_of_images=5, acc = metrics['accuracy'], h_dice = metrics['DICE'], alpha = 0.75, beta = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [tensor(98.6852),\n",
       "  tensor(98.8316),\n",
       "  tensor(99.2490),\n",
       "  tensor(99.2589),\n",
       "  tensor(99.1139)],\n",
       " 'soft DICE': [tensor(0.8779),\n",
       "  tensor(0.8455),\n",
       "  tensor(0.8958),\n",
       "  tensor(0.9120),\n",
       "  tensor(0.8862)],\n",
       " 'DICE': [tensor(0.8779),\n",
       "  tensor(0.8455),\n",
       "  tensor(0.8958),\n",
       "  tensor(0.9120),\n",
       "  tensor(0.8862)],\n",
       " 'time': [0.6579058170318604,\n",
       "  0.6233506202697754,\n",
       "  0.5815865993499756,\n",
       "  0.5859220027923584,\n",
       "  0.5579931735992432],\n",
       " 'means': [99.02773, 0.8834802, 0.8834802, 0.6013516426086426]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scarle pond 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametrs = {\n",
    "    'n_filters': 32,\n",
    "    'loss_weight': 0.8,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 50,\n",
    "    'lr_reduce_rate': 0.5,\n",
    "    'patience': 4,\n",
    "    'early_stopping': 50, # пока уберем раннюю остановку\n",
    "    'model': 'test'\n",
    "}\n",
    "model = UNet(n_filters=hyperparametrs['n_filters'])\n",
    "model.load_state_dict(torch.load('C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/weight/UNet_spinal-cord/scarlet-pond-23/weights.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "folder = 'C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/inference/UNet_spinal-cord/scarlet-pond-23/'\n",
    "\n",
    "metrics = save_predictions_as_imgs(test_loader, model, folder=folder)\n",
    "save_blended(path_to_folder=folder, number_of_images=5, acc = metrics['accuracy'], h_dice = metrics['DICE'], alpha = 0.75, beta = 0.8)\n",
    "\n",
    "\n",
    "# print(metrics)\n",
    "# print(\"mean accuracy: \", np.mean(metrics[0]))\n",
    "# print(\"mean DICE: \", np.mean(metrics[1]))\n",
    "\n",
    "# save_blended(path_to_folder=folder, number_of_images=5, acc = metrics[0], h_dice = metrics[1], alpha = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [tensor(99.1911),\n",
       "  tensor(98.7632),\n",
       "  tensor(99.3662),\n",
       "  tensor(99.1287),\n",
       "  tensor(99.1074)],\n",
       " 'soft DICE': [tensor(0.9044),\n",
       "  tensor(0.8831),\n",
       "  tensor(0.9136),\n",
       "  tensor(0.8866),\n",
       "  tensor(0.8770)],\n",
       " 'DICE': [tensor(0.9044),\n",
       "  tensor(0.8831),\n",
       "  tensor(0.9136),\n",
       "  tensor(0.8866),\n",
       "  tensor(0.8770)],\n",
       " 'time': [2.183528184890747,\n",
       "  1.9615082740783691,\n",
       "  2.008281707763672,\n",
       "  1.9867489337921143,\n",
       "  2.0976715087890625],\n",
       " 'means': [99.11131, 0.89293754, 0.89293754, 2.047547721862793]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9953222751617439"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import json\n",
    "\n",
    "# print (json.dumps(metrics, indent=2))\n",
    "\n",
    "# print(metrics[\"DICE\"][0])\n",
    "\n",
    "12.3 - np.sum(metrics[\"time\"])\n",
    "\n",
    "# print(\"mean accuracy: \", np.mean(metrics[0]))\n",
    "# print(\"mean DICE: \", np.mean(metrics[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_blended(path_to_folder=folder, number_of_images=5, acc = metrics[0], h_dice = metrics[1], alpha = 0.75)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comic Wind 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametrs = {\n",
    "    'n_filters': 32,\n",
    "    'loss_weight': 0.8,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 50,\n",
    "    'lr_reduce_rate': 0.5,\n",
    "    'patience': 4,\n",
    "    'early_stopping': 50, # пока уберем раннюю остановку\n",
    "    'model': 'test'\n",
    "}\n",
    "model = UNet(n_filters=hyperparametrs['n_filters'])\n",
    "model.load_state_dict(torch.load('C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/weight/UNet_spinal-cord/comic-wind-42/weights.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "folder = 'C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/inference/UNet_spinal-cord/comic-wind-42/'\n",
    "\n",
    "metrics = save_predictions_as_imgs(test_loader, model, folder=folder)\n",
    "\n",
    "save_blended(path_to_folder=folder, number_of_images=5, acc = metrics['accuracy'], h_dice = metrics['DICE'], alpha = 0.75, beta = 0.8)\n",
    "\n",
    "\n",
    "# print(metrics)\n",
    "# print(\"mean accuracy: \", np.mean(metrics[0]))\n",
    "# print(\"mean DICE: \", np.mean(metrics[1]))\n",
    "\n",
    "# save_blended(path_to_folder=folder, number_of_images=5, acc = metrics[0], h_dice = metrics[1], alpha = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [tensor(99.0889),\n",
       "  tensor(98.7078),\n",
       "  tensor(99.5531),\n",
       "  tensor(99.0107),\n",
       "  tensor(99.1740)],\n",
       " 'soft DICE': [tensor(0.8825),\n",
       "  tensor(0.8652),\n",
       "  tensor(0.9369),\n",
       "  tensor(0.8545),\n",
       "  tensor(0.8874)],\n",
       " 'DICE': [tensor(0.8825),\n",
       "  tensor(0.8652),\n",
       "  tensor(0.9369),\n",
       "  tensor(0.8545),\n",
       "  tensor(0.8874)],\n",
       " 'time': [9.05510401725769,\n",
       "  10.356470108032227,\n",
       "  9.091233253479004,\n",
       "  8.375613689422607,\n",
       "  8.400790214538574],\n",
       " 'means': [99.10689, 0.8852884, 0.8852884, 9.05584225654602]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apricot Water 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023-02-22_02:40\n",
    "\n",
    "hyperparametrs = {\n",
    "    'n_filters': 32,\n",
    "    'loss_weight': 0.8,\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 50,\n",
    "    'lr_reduce_rate': 0.5,\n",
    "    'patience': 4,\n",
    "    'early_stopping': 50, # пока уберем раннюю остановку\n",
    "    'model': 'test'\n",
    "}\n",
    "model = UNet(n_filters=hyperparametrs['n_filters'])\n",
    "model.load_state_dict(torch.load('C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/weight/UNet_spinal-cord/2023-02-22_02-40/weights.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "folder = 'C:/Users/gieko/Dropbox/NIITO_Vertebrae/Scripts/inference/UNet_spinal-cord/apricot-water-43/'\n",
    "\n",
    "metrics = save_predictions_as_imgs(test_loader, model, folder=folder)\n",
    "\n",
    "# print(metrics)\n",
    "# print(\"mean accuracy: \", np.mean(metrics[0]))\n",
    "# print(\"mean DICE: \", np.mean(metrics[1]))\n",
    "\n",
    "save_blended(path_to_folder=folder, number_of_images=5, acc = metrics['accuracy'], h_dice = metrics['DICE'], alpha = 0.75, beta = 0.8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "384b21feacb641c3e0314271197cb6efcbf5df7640640373c033a2a56e65c0f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
