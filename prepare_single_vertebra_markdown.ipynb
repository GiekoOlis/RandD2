{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The notebook for creating single vertebrae dataset out of full spine dataset\n",
    "\n",
    "## 1.1 Description of the original labels\n",
    "\n",
    "* Original markdown data from 3D Slicer contains a lot of information not necessary for ground truth creation and use. \n",
    "\n",
    "* Also it contains markdown for full spine, while what we need now is a set of markdowns of each vertebrae in a spine.\n",
    "\n",
    "* Run this notebook on original (non-resized data)\n",
    "\n",
    "Either we can firstly change the original markdown, in according to resized images (like, substract specific amount om millimeters from each point coordinates in markdown) and only after that proceed to this notebook and create data for yolo without any difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from functionality import *\n",
    "from operator import itemgetter\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw, ImageColor, ImageFont\n",
    "import pydicom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_dataset = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_resized\"\n",
    "path_to_dataset = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_Test_resized\"\n",
    "\n",
    "path_to_images = os.path.join(path_to_dataset, \"dicom\")\n",
    "path_to_labels = os.path.join(path_to_dataset, \"labels\")\n",
    "\n",
    "# path_to_image_001 = os.path.join(path_to_images, \"001\")\n",
    "# path_to_labels_001 = os.path.join(path_to_labels, *[\"001\", \"001_SD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['053_SD', '057_SD', '060_SD', '063_SD', '073_SD']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cases = os.listdir(path_to_labels)\n",
    "# idexes = [1,2,5,6,7,8,9,10,18,20,21,31,40,45]\n",
    "# cases[2]\n",
    "# kek = [ cases[i] for i in idexes]\n",
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from alive_progress import alive_bar\n",
    "palette_color = {\n",
    "    \"C\": \"#ff0000\",\n",
    "    \"T\": \"#ff7128\",\n",
    "    \"L\": \"#ffcc00\",\n",
    "    \"S\": \"#92d050\",\n",
    "    \"F\": \"#00b0f0\"\n",
    "}\n",
    "\n",
    "cases = os.listdir(path_to_labels)\n",
    "cases\n",
    "# cases = [\"001\"]\n",
    "\n",
    "show_exaple = True\n",
    "data_coordinates = []\n",
    "\n",
    "for case in cases:\n",
    "    path_to_orig_image = os.path.join(path_to_images, case + \".dcm\")\n",
    "    point_names = [\"CRV\", \"CAV\", \"CRD\", \"CAD\"]\n",
    "    # print(case, path_to_orig_image)\n",
    "    spacing = None\n",
    "\n",
    "    dicom = pydicom.dcmread(path_to_orig_image)\n",
    "    input = Image.fromarray(dicom.pixel_array)\n",
    "    input = input.convert('RGB')\n",
    "    # input = get_PIL_image(dicom)\n",
    "    # print(input)\n",
    "    # plt.figure(figsize=(50, 30))\n",
    "    if spacing == None:\n",
    "        spacing = dicom.ImagerPixelSpacing\n",
    "    page_width, page_height = input.size\n",
    "    markdowns = read_all_markdowns(os.path.join(path_to_labels, case))\n",
    "    # mask = Image.new('RGB', (width, height), 'black')\n",
    "    palette = palette_color\n",
    "    # draw = ImageDraw.Draw(input)\n",
    "    for k, markdown in enumerate(markdowns):\n",
    "        filename = case + \".png\"\n",
    "        id = k\n",
    "        AuthorID, Overlapped = ('NA', 'TRUE')\n",
    "\n",
    "        if \"FH\"  not in markdown['name']:\n",
    "            category = 'Vertebrae' if \"C2\" not in markdown['name'] else 'C2'\n",
    "            coordinates = [(float(i['position'][0]) / spacing[0], float(i['position'][1]) / spacing[1]) for i in markdown['controlPoints']]\n",
    "\n",
    "            min_x, max_x = min(coordinates, key=itemgetter(0))[0], max(coordinates, key=itemgetter(0))[0]\n",
    "            min_y, max_y = min(coordinates, key=itemgetter(1))[1], max(coordinates, key=itemgetter(1))[1]\n",
    "\n",
    "        else:\n",
    "            category = 'FermutHead'\n",
    "            coordinates = [(float(i['position'][0]), float(i['position'][1])) for i in markdown['controlPoints']]\n",
    "            c, r = define_circle(coordinates[0], coordinates[1], coordinates[2])\n",
    "            c, r = (c[0] / spacing[0], c[1] / spacing[1]), r / spacing[0]\n",
    "\n",
    "            min_x, max_x = c[0] - r, c[0] + r\n",
    "            min_y, max_y = c[1] - r, c[1] + r\n",
    "        \n",
    "        category1 = markdown['name']\n",
    "        width = max_x - min_x\n",
    "        height = max_y - min_y\n",
    "        x, y = int(min_x), int(min_y)\n",
    "        w, h = int(width), int(height)\n",
    "\n",
    "    #     # coordinates1 = [(min_x - 2, min_y - 2), (min_x - 2, max_y + 2), (max_x + 2, max_y + 2), (max_x + 2, min_y - 2)]\n",
    "        coordinates2 = [(min_x - 2, min_y - 2), (min_x - 2, min_y + h + 2), (min_x + w + 2, min_y + h + 2), (min_x + w + 2, min_y - 2)]\n",
    "\n",
    "   \n",
    "\n",
    "    #     # print(min_x, max_x, min_y, max_y)\n",
    "    #     # print(x, y, w, h)\n",
    "        # draw.polygon(tuple(coordinates), outline = \"white\", width=1)\n",
    "\n",
    "        # points = [x for x in markdown['controlPoints'] if x['label'] in [\"CRV\", \"CAV\", \"CRD\", \"CAD\"]]\n",
    "        # coordinates_points = [(float(i['position'][0] / spacing[0]), float(i['position'][1]) / spacing[1]) for i in points]\n",
    "        # point_size = 5\n",
    "        # for point in coordinates_points:\n",
    "        #     draw.ellipse([(point[0]-point_size, point[1]-point_size), (point[0]+point_size, point[1]+point_size)], fill=\"blue\", width=1)\n",
    "\n",
    "        # draw.polygon(tuple(coordinates2), outline = palette[markdown['name'][0]], width=2)\n",
    "        # font = ImageFont.truetype('arial', size=22)\n",
    "        # draw.text(\n",
    "        #     (min_x + w + 30, min_y),  # Coordinates\n",
    "        #     case + \" \" + markdown['name'],  # Text\n",
    "        #     # markdown['name'],  # Text\n",
    "        #     palette[markdown['name'][0]],  # Color\n",
    "        #     font\n",
    "        # )\n",
    "\n",
    "\n",
    "        row = [filename, page_height, page_width, AuthorID, Overlapped, category, id, x, y, w, h]\n",
    "        row1 = [filename, page_height, page_width, category, category1, x, y, w, h]\n",
    "\n",
    "        # data_coordinates.append(row)\n",
    "        data_coordinates.append(row1)\n",
    "\n",
    "    # plt.subplot(1, 2, 2)     \n",
    "    # plt.imshow(input)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create crops for U-Net Single Vertebra \n",
    "Also save original coordinates of right top corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_coordinates\n",
    "import pandas as pd\n",
    "documents = pd.DataFrame(data_coordinates, columns=['filename', 'page_height', 'page_width', 'category', 'region', 'x', 'y', 'width', 'height'])\n",
    "cases = documents['filename'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_source_dataset = os.path.join(path_to_dataset, \"png\")\n",
    "path_to_source_dataset_label = os.path.join(path_to_dataset, \"fill_no_FH\")\n",
    "path_to_source_dataset_dicom = os.path.join(path_to_dataset, \"dicom\")\n",
    "\n",
    "path_to_destination_folder = os.path.join(path_to_dataset, \"data_single_vertebra\")\n",
    "\n",
    "\n",
    "for case in cases:\n",
    "    path_to_case_png = os.path.join(path_to_source_dataset, case)\n",
    "    path_to_mask_png = os.path.join(path_to_source_dataset_label, case)\n",
    "\n",
    "    markdowns = read_all_markdowns(os.path.join(path_to_dataset, *[\"labels\", case.split(\".\")[0]]))\n",
    "\n",
    "    image = cv2.imread(path_to_case_png, 1)\n",
    "    dicom = pydicom.dcmread(os.path.join(path_to_source_dataset_dicom, case.split(\".\")[0] + \".dcm\"))\n",
    "    if spacing == None:\n",
    "        spacing = dicom.ImagerPixelSpacing\n",
    "    page_height, page_width = image.shape[:2]\n",
    "    crops = documents[(documents.filename == case) & (documents.category == \"Vertebrae\")]\n",
    "\n",
    "    for index, row in crops.iterrows():\n",
    "        # print(row.filename.split(\".\")[0] + \"_\" + row.region)\n",
    "        x, y = row.x, row.y\n",
    "        w, h = row.width, row.height\n",
    "        add = 15\n",
    "        mask = Image.new('RGB', (page_width, page_height), 'black')\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "\n",
    "        crop = image[y - add: y + h  + add, x - add: x + w + add]\n",
    "        new_file_path = os.path.join(*[path_to_destination_folder, \"images\", row.filename.split(\".\")[0] + \"_\" + row.region + \".png\"])\n",
    "        cv2.imwrite(new_file_path, crop)\n",
    "\n",
    "        vert = next(item for item in markdowns if item[\"name\"] == row.region)\n",
    "        coordinates = [(float(i['position'][0] / spacing[0]), float(i['position'][1]) / spacing[1]) for i in vert[\"controlPoints\"]]\n",
    "        draw.polygon(tuple(coordinates), fill='white', outline = 'white', width=1)\n",
    "        crop_mask = np.asarray(mask)\n",
    "        crop_mask = crop_mask[y - add: y + h  + add, x - add: x + w + add]\n",
    "        new_file_path_mask = os.path.join(*[path_to_destination_folder, \"labels\", row.filename.split(\".\")[0] + \"_\" + row.region + \".png\"])\n",
    "        cv2.imwrite(new_file_path_mask, crop_mask) \n",
    "\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting data to YOLOv5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "documents = pd.DataFrame(data_coordinates, columns=['filename', 'page_height', 'page_width', 'AuthorID', 'Overlapped', 'category', 'id', 'x', 'y', 'width', 'height'])\n",
    "documents\n",
    "\n",
    "\n",
    "# path_to_save = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_resized\\\\data_for_YOLO\\\\for_YOLO_Test.csv\"\n",
    "# documents[['filename', 'page_height', 'page_width', 'category', 'id', 'x', 'y', 'width', 'height', 'x_scaled', 'y_scaled', 'w_scaled', 'h_scaled']].to_csv(path_to_save, index=False, quotechar = ';')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QEMzPR0ylmEI"
   },
   "source": [
    "**Scaling the image to reduce training time**  \n",
    "To save on training time, resize the images to a maximum height and width of 640 and 273. While resizing the image, the bounding box cordinates also changes. This code computes how much each image is shrinken and updates the bounding box coordinates appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNdN0dD3lxmI"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# path_to_dataset_png = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_resized\\\\png\"\n",
    "# path_to_dataset_png_resized = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_resized\\\\png_h640_w273\"\n",
    "\n",
    "path_to_dataset_png = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_Test_resized\\\\png\"\n",
    "path_to_dataset_png_resized = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_Test_resized\"\n",
    "\n",
    "# path_to_documents_scaled = '/content/documents/documents/images'\n",
    "# path_to_ids_scaled = '/content/documents/ids/images'\n",
    "# path_to_laminated_scaled = '/content/documents/laminated/images'\n",
    "\n",
    "def scale_image(df, path_to_source_folder, path_to_destination_folder):\n",
    "    df_new = []\n",
    "    filename = df.filename\n",
    "    # page_height = df.page_height[0]\n",
    "    X, Y, W, H = map(int, df.x), map(int, df.y), map(int, df.width), map(int, df.height)\n",
    "    for file, x, y, w, h in zip(filename, X, Y, W, H):\n",
    "        image_path = os.path.join(path_to_source_folder, file)\n",
    "        img = cv2.imread(image_path, 1)\n",
    "        page_height, page_width = img.shape[:2]\n",
    "        # print(img.shape, page_height)\n",
    "        img = img[: page_height - 107, :]\n",
    "        page_height, page_width = img.shape[:2]\n",
    "        # print(img.shape, page_height, page_width)\n",
    "\n",
    "        max_height = 640\n",
    "        max_width = 273\n",
    "\n",
    "        # print(page_height, page_width)\n",
    "        \n",
    "        # computes the scaling factor\n",
    "        if max_height < page_height or max_width < page_width:\n",
    "            scaling_factor = max_height / float(page_height)\n",
    "            # print(\"h: \", scaling_factor)\n",
    "            if max_width/float(page_width) < scaling_factor:\n",
    "                scaling_factor = max_width / float(page_width)\n",
    "                # print(\"w: \", scaling_factor)\n",
    "            # scale the image with the scaling factor\n",
    "            img = cv2.resize(img, None, fx=scaling_factor, fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
    "            # print(\"@!\", img.shape)\n",
    "        jpg_filename = file.replace(file.split(\".\")[-1], 'jpg')\n",
    "        new_file_path = os.path.join(path_to_destination_folder, jpg_filename)\n",
    "        cv2.imwrite(new_file_path, img) # write the scales image\n",
    "        \n",
    "        # save new page height and width\n",
    "        page_height, page_width = page_height*scaling_factor, page_width*scaling_factor\n",
    "        # compute new x, y, w, h coordinates after scaling\n",
    "        x, y, w, h= int(x*scaling_factor), int(y*scaling_factor), int(w*scaling_factor), int(h*scaling_factor)\n",
    "        row = [jpg_filename, x, y, w, h, round(page_height), page_width]\n",
    "        df_new.append(row)\n",
    "    return df_new\n",
    "\n",
    "scaled_data = scale_image(documents, path_to_dataset_png, path_to_dataset_png_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_abt_scaling_to_df(scaled_data, original_df):\n",
    "  scaled = list(zip(*scaled_data))\n",
    "\n",
    "  original_df['new_filename'] = scaled[0]\n",
    "  original_df['x_scaled'] = scaled[1]\n",
    "  original_df['y_scaled'] = scaled[2]\n",
    "  original_df['w_scaled'] = scaled[3]\n",
    "  original_df['h_scaled'] = scaled[4]\n",
    "  original_df['page_height_scaled'] = scaled[5]\n",
    "  original_df['page_width_scaled'] = scaled[6]\n",
    "  return original_df\n",
    "\n",
    "\n",
    "add_data_abt_scaling_to_df(scaled_data, documents)\n",
    "\n",
    "path_to_save = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_resized\\\\data_for_YOLO\\\\for_YOLO_Test.csv\"\n",
    "\n",
    "documents[['filename', 'new_filename', 'page_height', 'page_width', 'page_height_scaled', 'page_width_scaled', 'AuthorID', 'Overlapped', 'category', 'id', 'x', 'y', 'width', 'height', 'x_scaled', 'y_scaled', 'w_scaled', 'h_scaled']].to_csv(path_to_save, index=False, quotechar = ';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_1 = {\n",
    "    \"C2\": 0,\n",
    "    \"Vertebrae\": 1,\n",
    "    \"FerumHead\": 2\n",
    "}\n",
    "\n",
    "classes_2 = {\n",
    "    \"C\": 0,\n",
    "    \"Th\": 1,\n",
    "    \"L\": 2,\n",
    "    \"S\": 3,\n",
    "    \"FH\": 4\n",
    "}\n",
    "\n",
    "\n",
    "def x_center(df):\n",
    "  return int(df.x_scaled + (df.w_scaled/2))\n",
    "def y_center(df):\n",
    "  return int(df.y_scaled + (df.h_scaled/2))\n",
    "\n",
    "def w_norm(df, col):\n",
    "  return df[col]/df['page_width_scaled']\n",
    "def h_norm(df, col):\n",
    "  return df[col]/df['page_height_scaled']\n",
    "\n",
    "def data_to_yolo(path_to_cleaned_csv):\n",
    "  df = pd.read_csv(path_to_cleaned_csv, sep=';')\n",
    "  \n",
    "  df['labels'] = [classes_1[df['category'][i]] for i in range(len(df))]\n",
    "\n",
    "  df['x_center'] = df.apply(x_center, axis=1)\n",
    "  df['y_center'] = df.apply(y_center, axis=1)\n",
    "\n",
    "  df['x_center_norm'] = df.apply(w_norm, col='x_center',axis=1)\n",
    "  df['width_norm'] = df.apply(w_norm, col='w_scaled', axis=1)\n",
    "\n",
    "  df['y_center_norm'] = df.apply(h_norm, col='y_center',axis=1)\n",
    "  df['height_norm'] = df.apply(h_norm, col='h_scaled',axis=1)\n",
    "  return df\n",
    "\n",
    "documents_yolo = data_to_yolo(path_to_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRlMOrc00Vmd"
   },
   "source": [
    "**Segregate labels to destination folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-2D1ATWzskO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300 50\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "def segregate_labels(df, train_label_path):\n",
    "  filenames = []\n",
    "  for filename in df.filename:\n",
    "    filenames.append(filename)\n",
    "  filenames = set(filenames)\n",
    "  for i, filename in enumerate(filenames):\n",
    "    # print(i)\n",
    "    name = filename.replace(filename.split('.')[-1], 'txt')\n",
    "    yolo_list = []\n",
    "\n",
    "    for _,row in df[df.filename == filename].iterrows():\n",
    "      yolo_list.append([row.labels, row.x_center_norm, row.y_center_norm, row.width_norm, row.height_norm])\n",
    "\n",
    "    yolo_list = np.array(yolo_list)\n",
    "    # Save the .img & .txt files to the corresponding train and validation folders\n",
    "    np.savetxt(os.path.join(train_label_path, name), yolo_list, fmt=[\"%d\", \"%f\", \"%f\", \"%f\", \"%f\"])\n",
    "path = \"C:\\\\Users\\\\gieko\\\\Dropbox\\\\NIITO_Vertebrae\\\\NIITO_Vertebrae_Dataset\\\\NIITO_Vertebrae_Dataset_Final_resized\\\\data_for_YOLO\\\\labels\"\n",
    "segregate_labels(documents_yolo, path)\n",
    "\n",
    "print(len(documents_yolo), len(os.listdir(path)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "384b21feacb641c3e0314271197cb6efcbf5df7640640373c033a2a56e65c0f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
